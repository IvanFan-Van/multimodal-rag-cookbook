{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search on WikiArt Dataset using CLIP\n",
    "\n",
    "This notebook demonstrates how to implement semantic search functionality using the CLIP model on the WikiArt dataset. We'll implement both text-to-image and image-to-image search capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "First, let's install and import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install torch torchvision open_clip_torch Pillow numpy pandas matplotlib tqdm requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLIP Model\n",
    "We'll use the OpenCLIP implementation and load a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the model and processor\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare WikiArt Dataset\n",
    "We'll download a subset of the WikiArt dataset and prepare it for our semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Create directories for dataset\n",
    "data_dir = Path('wikiart_data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download sample WikiArt metadata\n",
    "# Note: In a real implementation, you would need to obtain the actual WikiArt dataset\n",
    "# This is a placeholder for demonstration purposes\n",
    "sample_data = {\n",
    "    'filename': ['sample1.jpg', 'sample2.jpg'],\n",
    "    'url': ['https://example.com/sample1.jpg', 'https://example.com/sample2.jpg'],\n",
    "    'artist': ['Artist1', 'Artist2'],\n",
    "    'title': ['Artwork1', 'Artwork2']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Download images\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    save_path = data_dir / row['filename']\n",
    "    if not save_path.exists():\n",
    "        download_image(row['url'], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Extract and store CLIP features for all images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(image)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Extract features for all images\n",
    "image_features = {}\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    image_path = data_dir / row['filename']\n",
    "    if image_path.exists():\n",
    "        features = extract_features(image_path)\n",
    "        image_features[row['filename']] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Search Functions\n",
    "Create functions for both text-to-image and image-to-image search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def text_to_image_search(text_query, n_results=5):\n",
    "    # Encode text query\n",
    "    with torch.no_grad():\n",
    "        text = tokenizer(text_query).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features = text_features.cpu().numpy()\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {}\n",
    "    for filename, feat in image_features.items():\n",
    "        similarity = np.dot(text_features, feat.T)[0][0]\n",
    "        similarities[filename] = similarity\n",
    "    \n",
    "    # Sort and return top results\n",
    "    results = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:n_results]\n",
    "    return results\n",
    "\n",
    "def image_to_image_search(query_image_path, n_results=5):\n",
    "    # Extract features for query image\n",
    "    query_features = extract_features(query_image_path)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {}\n",
    "    for filename, feat in image_features.items():\n",
    "        similarity = np.dot(query_features, feat.T)[0][0]\n",
    "        similarities[filename] = similarity\n",
    "    \n",
    "    # Sort and return top results\n",
    "    results = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:n_results]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "Create functions to display search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def display_results(results):\n",
    "    n = len(results)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4*n, 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (filename, similarity) in zip(axes, results):\n",
    "        img = Image.open(data_dir / filename)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        title = f'{filename}\nSimilarity: {similarity:.3f}'\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Demonstrate how to use the semantic search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example text-to-image search\n",
    "text_query = \"a beautiful landscape painting with mountains\"\n",
    "results = text_to_image_search(text_query)\n",
    "print(f'Search results for: \"{text_query}\"')\n",
    "display_results(results)\n",
    "\n",
    "# Example image-to-image search\n",
    "query_image = data_dir / 'sample1.jpg'  # Replace with an actual image path\n",
    "results = image_to_image_search(query_image)\n",
    "print(f'Similar images to: {query_image.name}')\n",
    "display_results(results)"
   ]
  }
 ]
}